{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Hybrid Quantum Neural Network for dGmix Prediction\n",
    "\n",
    "This notebook implements a hybrid quantum-classical neural network using TensorFlow Quantum to predict dGmix values from the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, install the required packages. This is especially important in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow-quantum cirq sympy pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data in Google Colab\n",
    "\n",
    "There are two main methods to load your dataset in Google Colab:\n",
    "\n",
    "### Method 1: Direct Upload\n",
    "For smaller files (< 100MB), you can directly upload the file to the Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # This will prompt you to select and upload your file\n",
    "\n",
    "# After uploading, your file will be in the current directory\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# Display the first few rows of the uploaded dataset\n",
    "df = pd.read_csv('data_filtered-1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using Google Drive\n",
    "For larger files or if you want to persist data between sessions, mounting Google Drive is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Assuming your file is stored in a specific folder in Google Drive\n",
    "# Adjust the path as needed\n",
    "dataset_path = '/content/drive/MyDrive/your_folder/data_filtered-1.csv'\n",
    "\n",
    "# Read and display the first few rows\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Check TensorFlow and TFQ versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Quantum version: {tfq.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path):\n",
    "    \"\"\"Load and preprocess data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display initial information\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\")\n",
    "    \n",
    "    # Drop object columns\n",
    "    object_columns = df.select_dtypes(include=['object']).columns\n",
    "    print(f\"Dropping object columns: {object_columns.tolist()}\")\n",
    "    df = df.drop(columns=object_columns)\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_values = df.isna().sum().sum()\n",
    "    print(f\"Total missing values: {missing_values}\")\n",
    "    if missing_values > 0:\n",
    "        df = df.dropna()\n",
    "        print(f\"Shape after removing rows with missing values: {df.shape}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=['dGmix'])\n",
    "    y = df['dGmix']\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantum Circuit Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_quantum_circuit(qubits, features=None, n_layers=4):\n",
    "    \"\"\"Create a more advanced quantum circuit with feature encoding\"\"\"\n",
    "    circuit = cirq.Circuit()\n",
    "    \n",
    "    # Encode features if provided\n",
    "    if features is not None:\n",
    "        for i, feature in enumerate(features[:len(qubits)]):\n",
    "            circuit.append(cirq.rx(np.pi * feature).on(qubits[i]))\n",
    "            circuit.append(cirq.rz(np.pi * feature).on(qubits[i]))\n",
    "    \n",
    "    # Create variational parameters\n",
    "    params = []\n",
    "    symbols = []\n",
    "    \n",
    "    # Add variational layers\n",
    "    for l in range(n_layers):\n",
    "        for i in range(len(qubits)):\n",
    "            # Rotation gates\n",
    "            symbol_ry = sympy.Symbol(f'ry_{l}_{i}')\n",
    "            symbol_rz = sympy.Symbol(f'rz_{l}_{i}')\n",
    "            symbols.extend([symbol_ry, symbol_rz])\n",
    "            \n",
    "            circuit.append(cirq.ry(symbol_ry).on(qubits[i]))\n",
    "            circuit.append(cirq.rz(symbol_rz).on(qubits[i]))\n",
    "        \n",
    "        # Add entangling layer - more complex entanglement pattern\n",
    "        for i in range(len(qubits)):\n",
    "            circuit.append(cirq.CNOT(qubits[i], qubits[(i + 1) % len(qubits)]))\n",
    "            \n",
    "        # Add non-local interactions in deeper layers\n",
    "        if l >= n_layers // 2:\n",
    "            for i in range(0, len(qubits), 2):\n",
    "                if i + 2 < len(qubits):\n",
    "                    circuit.append(cirq.CNOT(qubits[i], qubits[i + 2]))\n",
    "    \n",
    "    return circuit, symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_advanced_hybrid_model(n_features, n_qubits, readout_qubits=None):\n",
    "    \"\"\"Build a more sophisticated hybrid quantum-classical model\"\"\"\n",
    "    if readout_qubits is None:\n",
    "        readout_qubits = list(range(n_qubits))\n",
    "    \n",
    "    # Create qubits\n",
    "    qubits = [cirq.GridQubit(0, i) for i in range(n_qubits)]\n",
    "    \n",
    "    # Create quantum circuit\n",
    "    circuit, symbols = create_advanced_quantum_circuit(qubits)\n",
    "    \n",
    "    # Define observables - use strategic combinations of qubits\n",
    "    observables = [cirq.Z(qubits[i]) for i in readout_qubits]\n",
    "    if len(readout_qubits) > 1:\n",
    "        # Add parity observables for better feature extraction\n",
    "        for i in range(len(readout_qubits) - 1):\n",
    "            observables.append(cirq.Z(qubits[readout_qubits[i]]) @ cirq.Z(qubits[readout_qubits[i+1]]))\n",
    "    \n",
    "    # Classical pre-processing\n",
    "    classical_input = tf.keras.layers.Input(shape=(n_features,), name='classical_input')\n",
    "    x = tf.keras.layers.BatchNormalization()(classical_input)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    encoded_classical = tf.keras.layers.Dense(n_qubits, activation='tanh')(x)\n",
    "    \n",
    "    # Quantum processing\n",
    "    quantum_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='quantum_input')\n",
    "    quantum_layer = tfq.layers.PQC(circuit, observables)(quantum_input)\n",
    "    \n",
    "    # Post-processing\n",
    "    combined = tf.keras.layers.Concatenate()([encoded_classical, quantum_layer])\n",
    "    x = tf.keras.layers.Dense(16, activation='relu')(combined)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=[classical_input, quantum_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your dataset\n",
    "# If using direct upload method:\n",
    "data_path = 'data_filtered-1.csv'\n",
    "\n",
    "# If using Google Drive method:\n",
    "# data_path = '/content/drive/MyDrive/your_folder/data_filtered-1.csv'\n",
    "\n",
    "# Load data\n",
    "X, y = prepare_data(data_path)\n",
    "print(f\"Data loaded: {X.shape} features, {y.shape} targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model parameters\n",
    "n_features = X.shape[1]\n",
    "n_qubits = min(n_features, 8)  # Limit qubits for efficiency\n",
    "readout_qubits = list(range(min(4, n_qubits)))  # Only read from a subset of qubits\n",
    "\n",
    "# Create qubits\n",
    "qubits = [cirq.GridQubit(0, i) for i in range(n_qubits)]\n",
    "\n",
    "print(f\"Using {n_qubits} qubits with {n_features} features\")\n",
    "print(f\"Readout qubits: {readout_qubits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement k-fold cross-validation for better evaluation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nTraining fold {fold+1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Scale to [0, 1] for quantum encoding\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_train_minmax = minmax_scaler.fit_transform(X_train_scaled)\n",
    "    X_test_minmax = minmax_scaler.transform(X_test_scaled)\n",
    "    \n",
    "    # Create quantum circuits for each data point (this can take some time)\n",
    "    print(\"Creating quantum circuits for training data...\")\n",
    "    X_train_circuits = [create_advanced_quantum_circuit(qubits, features=x)[0] for x in X_train_minmax]\n",
    "    \n",
    "    print(\"Creating quantum circuits for testing data...\")\n",
    "    X_test_circuits = [create_advanced_quantum_circuit(qubits, features=x)[0] for x in X_test_minmax]\n",
    "    \n",
    "    # Convert to TFQ tensors\n",
    "    print(\"Converting to TFQ tensors...\")\n",
    "    X_train_tfq = tfq.convert_to_tensor(X_train_circuits)\n",
    "    X_test_tfq = tfq.convert_to_tensor(X_test_circuits)\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building quantum model...\")\n",
    "    model = build_advanced_hybrid_model(n_features, n_qubits, readout_qubits)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(\n",
    "        [X_train_scaled, X_train_tfq],\n",
    "        y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Fold {fold+1} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title(f'Fold {fold+1} MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = model.predict([X_test_scaled, X_test_tfq])\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Fold {fold+1} - MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    fold_results.append((mse, r2))\n",
    "    \n",
    "    # Visualize predictions\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Fold {fold+1} - Predictions vs Actual (R² = {r2:.4f})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create directory for saving results\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    try:\n",
    "        # Try to save the model (may not work in Colab without mounting drive)\n",
    "        model.save(f'results/quantum_model_fold_{fold+1}')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save model: {str(e)}\")\n",
    "        print(\"To save models in Colab, you need to mount Google Drive and save to the drive path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Calculate Average Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average performance\n",
    "avg_mse = np.mean([res[0] for res in fold_results])\n",
    "avg_r2 = np.mean([res[1] for res in fold_results])\n",
    "print(f\"\\nAverage performance across {n_splits} folds:\")\n",
    "print(f\"Mean MSE: {avg_mse:.4f}\")\n",
    "print(f\"Mean R²: {avg_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Final Model on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all data\n",
    "print(\"\\nTraining final model on all data...\")\n",
    "\n",
    "# Standardize all features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Scale to [0, 1] for quantum encoding\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_minmax = minmax_scaler.fit_transform(X_scaled)\n",
    "\n",
    "# Create quantum circuits\n",
    "print(\"Creating quantum circuits for all data...\")\n",
    "X_circuits = [create_advanced_quantum_circuit(qubits, features=x)[0] for x in X_minmax]\n",
    "X_tfq = tfq.convert_to_tensor(X_circuits)\n",
    "\n",
    "# Build final model\n",
    "final_model = build_advanced_hybrid_model(n_features, n_qubits, readout_qubits)\n",
    "\n",
    "# Compile model\n",
    "final_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "final_history = final_model.fit(\n",
    "    [X_scaled, X_tfq],\n",
    "    y,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=8)\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Final Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(final_history.history['loss'])\n",
    "plt.plot(final_history.history['val_loss'])\n",
    "plt.title('Final Model Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(final_history.history['mae'])\n",
    "plt.plot(final_history.history['val_mae'])\n",
    "plt.title('Final Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model (to Google Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving to Google Drive\n",
    "try:\n",
    "    # Create a directory in Google Drive\n",
    "    drive_dir = '/content/drive/MyDrive/quantum_models'\n",
    "    os.makedirs(drive_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    final_model.save(f'{drive_dir}/final_quantum_model')\n",
    "    \n",
    "    # Save the visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(final_history.history['loss'])\n",
    "    plt.plot(final_history.history['val_loss'])\n",
    "    plt.title('Final Model Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.savefig(f'{drive_dir}/final_model_training.png')\n",
    "    \n",
    "    # Save the scalers for future prediction\n",
    "    import pickle\n",
    "    with open(f'{drive_dir}/standard_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(f'{drive_dir}/minmax_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(minmax_scaler, f)\n",
    "        \n",
    "    print(f\"Model and associated files saved to {drive_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to Google Drive: {str(e)}\")\n",
    "    print(\"Make sure you have mounted your Google Drive and have write permissions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Using the Model for Prediction (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the model for prediction\n",
    "def make_prediction(model, new_data, scaler, minmax_scaler, qubits):\n",
    "    \"\"\"Make predictions using the trained quantum model\"\"\"\n",
    "    # Scale the data\n",
    "    scaled_data = scaler.transform(new_data)\n",
    "    minmax_data = minmax_scaler.transform(scaled_data)\n",
    "    \n",
    "    # Create quantum circuits\n",
    "    circuits = [create_advanced_quantum_circuit(qubits, features=x)[0] for x in minmax_data]\n",
    "    tfq_data = tfq.convert_to_tensor(circuits)\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict([scaled_data, tfq_data])\n",
    "    return predictions\n",
    "\n",
    "# Example (assuming we have some new data)\n",
    "# new_data = X.iloc[0:5]  # Just for demonstration, using the first 5 rows\n",
    "# predictions = make_prediction(final_model, new_data, scaler, minmax_scaler, qubits)\n",
    "# print(\"Example predictions:\")\n",
    "# for i, pred in enumerate(predictions):\n",
    "#     print(f\"Sample {i+1}: Predicted dGmix = {pred[0]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
